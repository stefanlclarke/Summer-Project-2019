{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from neural_process import NeuralProcess, NeuralProcessImg\n",
    "from training import NeuralProcessTrainer\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torchvision.datasets as datasets\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# ...or for images\n",
    "neuralprocess = NeuralProcessImg(img_size=(1, 28, 28), r_dim=128, z_dim=128,\n",
    "                                 h_dim=128)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define optimizer and trainer\n",
    "optimizer = torch.optim.Adam(neuralprocess.parameters(), lr=3e-4)\n",
    "np_trainer = NeuralProcessTrainer(device, neuralprocess, optimizer,\n",
    "                                  num_context_range=(3, 20),\n",
    "                                  num_extra_target_range=(5, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import mnist, celeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader, _ = mnist(batch_size=32, size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAA+klEQVR4nO2OMY5FUBiF70zGAkhIFBK1Rm4hkUjEHtiCim1oLMAOrMIWbqNAiCgQIQqF5kq4pniZKR7JTOapJr7uPzn/lwPAzc2lvB0jy7Js2+66DmMchmHf92VZviqtqkoUxe9znuckSU6f27b1fR8h9JR/HKu2bcuynKapJEkQQsMwVFVtmkYQhEdhXddxHHmeBwDUdX2Unix9gqZpCCFCSFGUR4IxLooiyzKGYVzXDYLgR8mvME1z27Y4jhmGucbIcdwwDPu+m6Z5Wnj/g9RxHJZlp2nK8/y1eV9omrYsCyFE1/VrjAAAz/MIIVEUURR1mfTm5uZf8AmQqGRIzwp8IAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28 at 0x7F64E228CE10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a binary mask to occlude image. For Neural Processes,\n",
    "# the context points will be defined as the visible pixels\n",
    "context_mask = torch.zeros((28, 28)).byte()\n",
    "context_mask[:16, :] = 1  # Top half of pixels are visible\n",
    "\n",
    "img = mnist_testset[0][0]\n",
    "img = torch.from_numpy(np.asarray(img)).float()\n",
    "occluded_img = img * context_mask.float()\n",
    "Image.fromarray(occluded_img.numpy()).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100, loss 7.196\n",
      "iteration 200, loss 5.873\n",
      "iteration 300, loss 4.234\n",
      "iteration 400, loss 2.469\n",
      "iteration 500, loss 0.930\n",
      "iteration 600, loss -2.003\n",
      "iteration 700, loss -3.633\n",
      "iteration 800, loss -3.725\n",
      "iteration 900, loss -4.568\n",
      "iteration 1000, loss -13.097\n",
      "iteration 1100, loss -4.191\n",
      "iteration 1200, loss -10.338\n",
      "iteration 1300, loss -10.098\n",
      "iteration 1400, loss -10.645\n",
      "iteration 1500, loss -11.420\n",
      "iteration 1600, loss -15.493\n",
      "iteration 1700, loss -8.770\n",
      "iteration 1800, loss -11.359\n",
      "iteration 1900, loss -9.183\n",
      "iteration 2000, loss -10.302\n",
      "iteration 2100, loss -8.679\n",
      "iteration 2200, loss -13.780\n",
      "iteration 2300, loss -17.554\n",
      "iteration 2400, loss -10.197\n",
      "iteration 2500, loss -10.155\n",
      "iteration 2600, loss -12.608\n",
      "iteration 2700, loss -12.232\n",
      "iteration 2800, loss -4.297\n",
      "iteration 2900, loss -5.854\n",
      "iteration 3000, loss -7.726\n",
      "iteration 3100, loss -12.528\n",
      "iteration 3200, loss -10.244\n",
      "iteration 3300, loss -8.094\n",
      "iteration 3400, loss -10.452\n",
      "iteration 3500, loss -10.814\n",
      "iteration 3600, loss -7.063\n",
      "iteration 3700, loss -8.569\n",
      "iteration 3800, loss -14.219\n",
      "iteration 3900, loss -12.806\n",
      "iteration 4000, loss -8.001\n",
      "iteration 4100, loss -11.988\n",
      "iteration 4200, loss -4.772\n",
      "iteration 4300, loss -5.167\n",
      "iteration 4400, loss -11.341\n",
      "iteration 4500, loss -8.237\n",
      "iteration 4600, loss -9.214\n",
      "iteration 4700, loss -10.644\n",
      "iteration 4800, loss -11.985\n",
      "iteration 4900, loss -9.474\n",
      "iteration 5000, loss -10.412\n",
      "iteration 5100, loss -11.972\n",
      "iteration 5200, loss -15.282\n",
      "iteration 5300, loss -5.031\n",
      "iteration 5400, loss -7.212\n",
      "iteration 5500, loss -12.747\n",
      "iteration 5600, loss -9.197\n",
      "iteration 5700, loss -13.510\n",
      "iteration 5800, loss -7.568\n",
      "iteration 5900, loss -12.663\n",
      "iteration 6000, loss -14.068\n",
      "iteration 6100, loss -21.413\n",
      "iteration 6200, loss -5.137\n",
      "iteration 6300, loss -14.004\n",
      "iteration 6400, loss -10.426\n",
      "iteration 6500, loss -10.053\n",
      "iteration 6600, loss -6.685\n",
      "iteration 6700, loss -7.905\n",
      "iteration 6800, loss -8.220\n",
      "iteration 6900, loss -12.052\n",
      "iteration 7000, loss -13.901\n",
      "iteration 7100, loss -12.899\n",
      "iteration 7200, loss -13.431\n",
      "iteration 7300, loss -10.643\n",
      "iteration 7400, loss -6.302\n",
      "iteration 7500, loss -7.190\n",
      "iteration 7600, loss -11.047\n",
      "iteration 7700, loss -12.591\n",
      "iteration 7800, loss -9.070\n",
      "iteration 7900, loss -11.900\n",
      "iteration 8000, loss -10.877\n",
      "iteration 8100, loss -11.596\n",
      "iteration 8200, loss -13.366\n",
      "iteration 8300, loss -9.657\n",
      "iteration 8400, loss -13.838\n",
      "iteration 8500, loss -18.262\n",
      "iteration 8600, loss -14.998\n",
      "iteration 8700, loss -16.652\n",
      "iteration 8800, loss -16.808\n",
      "iteration 8900, loss -7.104\n",
      "iteration 9000, loss -8.239\n",
      "iteration 9100, loss -12.820\n",
      "iteration 9200, loss -15.014\n",
      "iteration 9300, loss -13.510\n",
      "iteration 9400, loss -14.923\n",
      "iteration 9500, loss -11.231\n",
      "iteration 9600, loss -16.627\n",
      "iteration 9700, loss -15.932\n",
      "iteration 9800, loss -6.067\n",
      "iteration 9900, loss -4.942\n",
      "iteration 10000, loss -6.682\n",
      "iteration 10100, loss -9.325\n",
      "iteration 10200, loss -15.503\n",
      "iteration 10300, loss -18.669\n",
      "iteration 10400, loss -14.244\n",
      "iteration 10500, loss -6.869\n",
      "iteration 10600, loss -14.205\n",
      "iteration 10700, loss -16.323\n",
      "iteration 10800, loss -11.151\n",
      "iteration 10900, loss -15.643\n",
      "iteration 11000, loss -12.565\n",
      "iteration 11100, loss -10.524\n",
      "iteration 11200, loss -10.019\n",
      "iteration 11300, loss -7.122\n",
      "iteration 11400, loss -14.278\n",
      "iteration 11500, loss -16.716\n",
      "iteration 11600, loss -7.375\n",
      "iteration 11700, loss -14.287\n",
      "iteration 11800, loss -19.703\n",
      "iteration 11900, loss -7.026\n",
      "iteration 12000, loss -8.842\n",
      "iteration 12100, loss -9.394\n",
      "iteration 12200, loss -13.674\n",
      "iteration 12300, loss -13.681\n",
      "iteration 12400, loss -13.494\n",
      "iteration 12500, loss -12.748\n",
      "iteration 12600, loss -17.339\n",
      "iteration 12700, loss -19.161\n",
      "iteration 12800, loss -8.845\n",
      "iteration 12900, loss -12.542\n",
      "iteration 13000, loss -12.147\n",
      "iteration 13100, loss -11.507\n",
      "iteration 13200, loss -16.194\n",
      "iteration 13300, loss -11.917\n",
      "iteration 13400, loss -9.138\n",
      "iteration 13500, loss -20.758\n",
      "iteration 13600, loss -17.644\n",
      "iteration 13700, loss -11.242\n",
      "iteration 13800, loss -10.866\n",
      "iteration 13900, loss -8.455\n",
      "iteration 14000, loss -13.632\n",
      "iteration 14100, loss -11.068\n",
      "iteration 14200, loss -14.868\n",
      "iteration 14300, loss -8.788\n",
      "iteration 14400, loss -13.075\n",
      "iteration 14500, loss -10.378\n",
      "iteration 14600, loss -7.611\n",
      "iteration 14700, loss -14.567\n",
      "iteration 14800, loss -18.177\n",
      "iteration 14900, loss -9.058\n",
      "iteration 15000, loss -10.898\n",
      "iteration 15100, loss -9.237\n",
      "iteration 15200, loss -12.033\n",
      "iteration 15300, loss -16.085\n",
      "iteration 15400, loss -10.394\n",
      "iteration 15500, loss -6.577\n",
      "iteration 15600, loss -5.046\n",
      "iteration 15700, loss -15.225\n",
      "iteration 15800, loss -8.721\n",
      "iteration 15900, loss -11.118\n",
      "iteration 16000, loss -17.125\n",
      "iteration 16100, loss -15.571\n",
      "iteration 16200, loss -13.068\n",
      "iteration 16300, loss -11.513\n",
      "iteration 16400, loss -7.876\n",
      "iteration 16500, loss -6.626\n",
      "iteration 16600, loss -14.633\n",
      "iteration 16700, loss -17.703\n",
      "iteration 16800, loss -10.093\n",
      "iteration 16900, loss -9.148\n",
      "iteration 17000, loss -12.586\n",
      "iteration 17100, loss -19.353\n",
      "iteration 17200, loss -16.112\n",
      "iteration 17300, loss -15.019\n",
      "iteration 17400, loss -9.546\n",
      "iteration 17500, loss -12.644\n",
      "iteration 17600, loss -11.826\n",
      "iteration 17700, loss -12.139\n",
      "iteration 17800, loss -10.427\n",
      "iteration 17900, loss -16.444\n",
      "iteration 18000, loss -17.668\n",
      "iteration 18100, loss -13.761\n",
      "iteration 18200, loss -15.994\n",
      "iteration 18300, loss -8.999\n",
      "iteration 18400, loss -9.915\n",
      "iteration 18500, loss -13.993\n",
      "iteration 18600, loss -12.688\n",
      "iteration 18700, loss -17.654\n",
      "iteration 18800, loss -17.082\n",
      "iteration 18900, loss -10.469\n",
      "iteration 19000, loss -12.714\n",
      "iteration 19100, loss -11.475\n",
      "iteration 19200, loss -20.509\n",
      "iteration 19300, loss -17.102\n",
      "iteration 19400, loss -12.502\n",
      "iteration 19500, loss -15.859\n",
      "iteration 19600, loss -7.625\n",
      "iteration 19700, loss -8.878\n",
      "iteration 19800, loss -11.922\n",
      "iteration 19900, loss -18.604\n",
      "iteration 20000, loss -16.863\n",
      "iteration 20100, loss -10.894\n",
      "iteration 20200, loss -13.595\n",
      "iteration 20300, loss -16.591\n",
      "iteration 20400, loss -15.043\n",
      "iteration 20500, loss -10.659\n",
      "iteration 20600, loss -14.406\n",
      "iteration 20700, loss -19.518\n",
      "iteration 20800, loss -13.472\n",
      "iteration 20900, loss -6.531\n",
      "iteration 21000, loss -13.735\n",
      "iteration 21100, loss -7.955\n",
      "iteration 21200, loss -12.570\n",
      "iteration 21300, loss -14.041\n",
      "iteration 21400, loss -14.037\n",
      "iteration 21500, loss -12.404\n",
      "iteration 21600, loss -18.452\n",
      "iteration 21700, loss -8.574\n",
      "iteration 21800, loss -8.288\n",
      "iteration 21900, loss -14.126\n",
      "iteration 22000, loss -9.525\n",
      "iteration 22100, loss -17.139\n",
      "iteration 22200, loss -13.677\n",
      "iteration 22300, loss -18.158\n",
      "iteration 22400, loss -13.561\n",
      "iteration 22500, loss -5.032\n",
      "iteration 22600, loss -8.905\n",
      "iteration 22700, loss -9.334\n",
      "iteration 22800, loss -10.526\n",
      "iteration 22900, loss -15.294\n",
      "iteration 23000, loss -18.168\n",
      "iteration 23100, loss -16.788\n",
      "iteration 23200, loss -9.452\n",
      "iteration 23300, loss -10.689\n",
      "iteration 23400, loss -13.058\n",
      "iteration 23500, loss -19.364\n",
      "iteration 23600, loss -9.344\n",
      "iteration 23700, loss -16.482\n",
      "iteration 23800, loss -10.262\n",
      "iteration 23900, loss -11.731\n",
      "iteration 24000, loss -19.266\n",
      "iteration 24100, loss -17.100\n",
      "iteration 24200, loss -17.668\n",
      "iteration 24300, loss -14.116\n",
      "iteration 24400, loss -17.920\n",
      "iteration 24500, loss -8.132\n",
      "iteration 24600, loss -7.637\n",
      "iteration 24700, loss -14.165\n",
      "iteration 24800, loss -14.048\n",
      "iteration 24900, loss -21.113\n",
      "iteration 25000, loss -4.574\n",
      "iteration 25100, loss -14.434\n",
      "iteration 25200, loss -9.418\n",
      "iteration 25300, loss -12.485\n",
      "iteration 25400, loss -8.583\n",
      "iteration 25500, loss -10.174\n",
      "iteration 25600, loss -21.912\n",
      "iteration 25700, loss -13.632\n",
      "iteration 25800, loss -15.225\n",
      "iteration 25900, loss -11.765\n",
      "iteration 26000, loss -5.538\n",
      "iteration 26100, loss -9.872\n",
      "iteration 26200, loss -6.550\n",
      "iteration 26300, loss -14.072\n",
      "iteration 26400, loss -11.823\n",
      "iteration 26500, loss -16.385\n",
      "iteration 26600, loss -15.291\n",
      "iteration 26700, loss -5.852\n",
      "iteration 26800, loss -16.049\n",
      "iteration 26900, loss -10.044\n",
      "iteration 27000, loss -13.419\n",
      "iteration 27100, loss -12.134\n",
      "iteration 27200, loss -10.996\n",
      "iteration 27300, loss -16.366\n",
      "iteration 27400, loss -12.704\n",
      "iteration 27500, loss -14.249\n",
      "iteration 27600, loss -8.794\n",
      "iteration 27700, loss -14.268\n",
      "iteration 27800, loss -20.971\n",
      "iteration 27900, loss -6.048\n",
      "iteration 28000, loss -13.916\n",
      "iteration 28100, loss -16.029\n",
      "iteration 28200, loss -4.769\n",
      "iteration 28300, loss -5.759\n",
      "iteration 28400, loss -17.216\n",
      "iteration 28500, loss -12.558\n",
      "iteration 28600, loss -16.428\n",
      "iteration 28700, loss -6.947\n",
      "iteration 28800, loss -18.777\n",
      "iteration 28900, loss -13.508\n",
      "iteration 29000, loss -17.607\n",
      "iteration 29100, loss -14.431\n",
      "iteration 29200, loss -16.663\n",
      "iteration 29300, loss -10.241\n",
      "iteration 29400, loss -9.737\n",
      "iteration 29500, loss -17.982\n",
      "iteration 29600, loss -18.112\n",
      "iteration 29700, loss -15.748\n",
      "iteration 29800, loss -11.982\n",
      "iteration 29900, loss -12.095\n",
      "iteration 30000, loss -17.063\n",
      "iteration 30100, loss -7.875\n",
      "iteration 30200, loss -9.169\n",
      "iteration 30300, loss -20.501\n",
      "iteration 30400, loss -11.787\n",
      "iteration 30500, loss -18.331\n",
      "iteration 30600, loss -13.504\n",
      "iteration 30700, loss -14.894\n",
      "iteration 30800, loss -15.654\n",
      "iteration 30900, loss -17.030\n",
      "iteration 31000, loss -13.833\n",
      "iteration 31100, loss -8.583\n",
      "iteration 31200, loss -7.506\n",
      "iteration 31300, loss -9.487\n",
      "iteration 31400, loss -15.388\n",
      "iteration 31500, loss -11.241\n",
      "iteration 31600, loss -9.303\n",
      "iteration 31700, loss -14.921\n",
      "iteration 31800, loss -13.423\n",
      "iteration 31900, loss -14.351\n",
      "iteration 32000, loss -19.587\n",
      "iteration 32100, loss -17.494\n",
      "iteration 32200, loss -17.352\n",
      "iteration 32300, loss -15.311\n",
      "iteration 32400, loss -8.356\n",
      "iteration 32500, loss -11.932\n",
      "iteration 32600, loss -14.867\n",
      "iteration 32700, loss -18.994\n",
      "iteration 32800, loss -17.065\n",
      "iteration 32900, loss -14.364\n",
      "iteration 33000, loss -5.805\n",
      "iteration 33100, loss -16.291\n",
      "iteration 33200, loss -7.214\n",
      "iteration 33300, loss -11.346\n",
      "iteration 33400, loss -14.075\n",
      "iteration 33500, loss -5.822\n",
      "iteration 33600, loss -18.949\n",
      "iteration 33700, loss -13.335\n",
      "iteration 33800, loss -7.793\n",
      "iteration 33900, loss -10.605\n",
      "iteration 34000, loss -12.210\n",
      "iteration 34100, loss -12.748\n",
      "iteration 34200, loss -16.665\n",
      "iteration 34300, loss -15.970\n",
      "iteration 34400, loss -13.574\n",
      "iteration 34500, loss -16.668\n",
      "iteration 34600, loss -19.976\n",
      "iteration 34700, loss -19.253\n",
      "iteration 34800, loss -7.193\n",
      "iteration 34900, loss -17.115\n",
      "iteration 35000, loss -15.277\n",
      "iteration 35100, loss -14.124\n",
      "iteration 35200, loss -8.659\n",
      "iteration 35300, loss -7.597\n",
      "iteration 35400, loss -5.289\n",
      "iteration 35500, loss -15.208\n",
      "iteration 35600, loss -15.874\n",
      "iteration 35700, loss -5.327\n",
      "iteration 35800, loss -10.989\n",
      "iteration 35900, loss -14.336\n",
      "iteration 36000, loss -13.958\n",
      "iteration 36100, loss -18.133\n",
      "iteration 36200, loss -13.978\n",
      "iteration 36300, loss -18.642\n",
      "iteration 36400, loss -19.558\n",
      "iteration 36500, loss -14.888\n",
      "iteration 36600, loss -21.676\n",
      "iteration 36700, loss -12.712\n",
      "iteration 36800, loss -5.961\n",
      "iteration 36900, loss -19.149\n",
      "iteration 37000, loss -11.661\n",
      "iteration 37100, loss -9.360\n",
      "iteration 37200, loss -11.017\n",
      "iteration 37300, loss -15.727\n",
      "iteration 37400, loss -15.826\n",
      "iteration 37500, loss -16.760\n",
      "iteration 37600, loss -14.374\n",
      "iteration 37700, loss -16.117\n",
      "iteration 37800, loss -8.761\n",
      "iteration 37900, loss -8.839\n",
      "iteration 38000, loss -19.591\n",
      "iteration 38100, loss -8.898\n",
      "iteration 38200, loss -13.231\n",
      "iteration 38300, loss -19.694\n",
      "iteration 38400, loss -17.314\n",
      "iteration 38500, loss -16.597\n",
      "iteration 38600, loss -17.434\n",
      "iteration 38700, loss -15.408\n",
      "iteration 38800, loss -15.889\n",
      "iteration 38900, loss -18.155\n",
      "iteration 39000, loss -8.806\n",
      "iteration 39100, loss -13.627\n",
      "iteration 39200, loss -13.385\n",
      "iteration 39300, loss -15.704\n",
      "iteration 39400, loss -13.241\n",
      "iteration 39500, loss -15.888\n",
      "iteration 39600, loss -16.123\n",
      "iteration 39700, loss -16.328\n",
      "iteration 39800, loss -8.133\n",
      "iteration 39900, loss -10.461\n",
      "iteration 40000, loss -13.427\n",
      "iteration 40100, loss -20.258\n",
      "iteration 40200, loss -9.935\n",
      "iteration 40300, loss -14.597\n",
      "iteration 40400, loss -6.373\n",
      "iteration 40500, loss -16.561\n",
      "iteration 40600, loss -9.183\n",
      "iteration 40700, loss -14.025\n",
      "iteration 40800, loss -11.706\n",
      "iteration 40900, loss -19.616\n",
      "iteration 41000, loss -16.900\n",
      "iteration 41100, loss -16.926\n",
      "iteration 41200, loss -20.861\n",
      "iteration 41300, loss -9.078\n",
      "iteration 41400, loss -14.517\n",
      "iteration 41500, loss -7.462\n",
      "iteration 41600, loss -13.664\n",
      "iteration 41700, loss -17.238\n",
      "iteration 41800, loss -13.097\n",
      "iteration 41900, loss -17.954\n",
      "iteration 42000, loss -5.887\n",
      "iteration 42100, loss -17.289\n",
      "iteration 42200, loss -16.287\n",
      "iteration 42300, loss -9.006\n",
      "iteration 42400, loss -9.862\n",
      "iteration 42500, loss -10.515\n",
      "iteration 42600, loss -17.048\n",
      "iteration 42700, loss -11.262\n",
      "iteration 42800, loss -20.562\n",
      "iteration 42900, loss -18.540\n",
      "iteration 43000, loss -18.675\n",
      "iteration 43100, loss -14.317\n",
      "iteration 43200, loss -15.683\n",
      "iteration 43300, loss -17.648\n",
      "iteration 43400, loss -13.368\n",
      "iteration 43500, loss -9.794\n",
      "iteration 43600, loss -14.624\n",
      "iteration 43700, loss -12.235\n",
      "iteration 43800, loss -12.149\n",
      "iteration 43900, loss -5.605\n",
      "iteration 44000, loss -19.900\n",
      "iteration 44100, loss -16.719\n",
      "iteration 44200, loss -7.764\n",
      "iteration 44300, loss -15.884\n",
      "iteration 44400, loss -18.524\n",
      "iteration 44500, loss -16.284\n",
      "iteration 44600, loss -7.811\n",
      "iteration 44700, loss -19.441\n",
      "iteration 44800, loss -16.134\n",
      "iteration 44900, loss -19.045\n",
      "iteration 45000, loss -7.139\n",
      "iteration 45100, loss -19.862\n",
      "iteration 45200, loss -17.493\n",
      "iteration 45300, loss -11.208\n",
      "iteration 45400, loss -9.084\n",
      "iteration 45500, loss -12.076\n",
      "iteration 45600, loss -9.960\n",
      "iteration 45700, loss -10.313\n",
      "iteration 45800, loss -16.944\n",
      "iteration 45900, loss -8.735\n",
      "iteration 46000, loss -14.132\n",
      "iteration 46100, loss -10.887\n",
      "iteration 46200, loss -14.276\n",
      "iteration 46300, loss -9.697\n",
      "iteration 46400, loss -22.652\n",
      "iteration 46500, loss -9.791\n",
      "iteration 46600, loss -12.915\n",
      "iteration 46700, loss -15.712\n",
      "iteration 46800, loss -16.327\n",
      "iteration 46900, loss -6.757\n",
      "iteration 47000, loss -21.814\n",
      "iteration 47100, loss -21.869\n",
      "iteration 47200, loss -16.608\n",
      "iteration 47300, loss -8.997\n",
      "iteration 47400, loss -16.086\n",
      "iteration 47500, loss -13.044\n",
      "iteration 47600, loss -5.286\n",
      "iteration 47700, loss -14.608\n",
      "iteration 47800, loss -10.605\n",
      "iteration 47900, loss -14.935\n",
      "iteration 48000, loss -12.892\n",
      "iteration 48100, loss -10.128\n",
      "iteration 48200, loss -7.614\n",
      "iteration 48300, loss -10.935\n",
      "iteration 48400, loss -12.364\n",
      "iteration 48500, loss -19.868\n",
      "iteration 48600, loss -10.790\n",
      "iteration 48700, loss -15.504\n",
      "iteration 48800, loss -5.476\n",
      "iteration 48900, loss -21.241\n",
      "iteration 49000, loss -14.987\n",
      "iteration 49100, loss -16.924\n",
      "iteration 49200, loss -7.223\n",
      "iteration 49300, loss -10.730\n",
      "iteration 49400, loss -15.377\n",
      "iteration 49500, loss -13.055\n",
      "iteration 49600, loss -8.155\n",
      "iteration 49700, loss -11.501\n",
      "iteration 49800, loss -16.310\n",
      "iteration 49900, loss -8.662\n",
      "iteration 50000, loss -17.243\n",
      "iteration 50100, loss -16.025\n",
      "iteration 50200, loss -19.637\n",
      "iteration 50300, loss -9.207\n",
      "iteration 50400, loss -18.699\n",
      "iteration 50500, loss -17.065\n",
      "iteration 50600, loss -10.900\n",
      "iteration 50700, loss -21.563\n",
      "iteration 50800, loss -17.459\n",
      "iteration 50900, loss -7.048\n",
      "iteration 51000, loss -17.836\n",
      "iteration 51100, loss -7.041\n",
      "iteration 51200, loss -18.148\n",
      "iteration 51300, loss -18.098\n",
      "iteration 51400, loss -13.078\n",
      "iteration 51500, loss -13.366\n",
      "iteration 51600, loss -11.166\n",
      "iteration 51700, loss -10.225\n",
      "iteration 51800, loss -15.219\n",
      "iteration 51900, loss -17.730\n",
      "iteration 52000, loss -20.428\n",
      "iteration 52100, loss -23.745\n",
      "iteration 52200, loss -14.964\n",
      "iteration 52300, loss -10.540\n",
      "iteration 52400, loss -13.137\n",
      "iteration 52500, loss -21.339\n",
      "iteration 52600, loss -11.969\n",
      "iteration 52700, loss -21.938\n",
      "iteration 52800, loss -7.430\n",
      "iteration 52900, loss -18.740\n",
      "iteration 53000, loss -17.136\n",
      "iteration 53100, loss -19.180\n",
      "iteration 53200, loss -11.761\n",
      "iteration 53300, loss -19.843\n",
      "iteration 53400, loss -17.847\n",
      "iteration 53500, loss -17.926\n",
      "iteration 53600, loss -9.046\n",
      "iteration 53700, loss -14.677\n",
      "iteration 53800, loss -14.118\n",
      "iteration 53900, loss -13.524\n",
      "iteration 54000, loss -7.749\n",
      "iteration 54100, loss -20.262\n",
      "iteration 54200, loss -13.979\n",
      "iteration 54300, loss -14.906\n",
      "iteration 54400, loss -17.657\n",
      "iteration 54500, loss -14.673\n",
      "iteration 54600, loss -20.458\n",
      "iteration 54700, loss -20.468\n",
      "iteration 54800, loss -14.042\n",
      "iteration 54900, loss -17.061\n",
      "iteration 55000, loss -18.064\n",
      "iteration 55100, loss -21.765\n",
      "iteration 55200, loss -21.141\n",
      "iteration 55300, loss -14.503\n",
      "iteration 55400, loss -20.563\n",
      "iteration 55500, loss -16.629\n",
      "iteration 55600, loss -16.345\n",
      "iteration 55700, loss -18.460\n",
      "iteration 55800, loss -16.649\n",
      "iteration 55900, loss -19.828\n",
      "iteration 56000, loss -21.194\n",
      "iteration 56100, loss -17.624\n",
      "iteration 56200, loss -11.708\n",
      "iteration 56300, loss -14.452\n",
      "iteration 56400, loss -16.770\n",
      "iteration 56500, loss -15.490\n",
      "iteration 56600, loss -9.251\n",
      "iteration 56700, loss -8.285\n",
      "iteration 56800, loss -9.579\n",
      "iteration 56900, loss -20.007\n",
      "iteration 57000, loss -6.663\n",
      "iteration 57100, loss -14.137\n",
      "iteration 57200, loss -12.850\n",
      "iteration 57300, loss -16.590\n",
      "iteration 57400, loss -11.776\n",
      "iteration 57500, loss -7.243\n",
      "iteration 57600, loss -8.355\n",
      "iteration 57700, loss -14.733\n",
      "iteration 57800, loss -13.735\n",
      "iteration 57900, loss -17.802\n",
      "iteration 58000, loss -13.838\n",
      "iteration 58100, loss -20.381\n",
      "iteration 58200, loss -16.224\n",
      "iteration 58300, loss -16.616\n",
      "iteration 58400, loss -18.908\n",
      "iteration 58500, loss -20.177\n",
      "iteration 58600, loss -16.785\n",
      "iteration 58700, loss -21.528\n",
      "iteration 58800, loss -13.831\n",
      "iteration 58900, loss -13.213\n",
      "iteration 59000, loss -12.049\n",
      "iteration 59100, loss -20.873\n",
      "iteration 59200, loss -14.462\n",
      "iteration 59300, loss -12.370\n",
      "iteration 59400, loss -16.925\n",
      "iteration 59500, loss -10.261\n",
      "iteration 59600, loss -17.416\n",
      "iteration 59700, loss -21.237\n",
      "iteration 59800, loss -18.169\n",
      "iteration 59900, loss -9.506\n",
      "iteration 60000, loss -6.335\n",
      "iteration 60100, loss -7.976\n",
      "iteration 60200, loss -19.047\n",
      "iteration 60300, loss -20.499\n",
      "iteration 60400, loss -10.395\n",
      "iteration 60500, loss -9.552\n",
      "iteration 60600, loss -17.146\n",
      "iteration 60700, loss -16.319\n",
      "iteration 60800, loss -10.073\n",
      "iteration 60900, loss -16.491\n",
      "iteration 61000, loss -18.836\n",
      "iteration 61100, loss -15.784\n",
      "iteration 61200, loss -11.408\n",
      "iteration 61300, loss -7.293\n",
      "iteration 61400, loss -19.888\n",
      "iteration 61500, loss -14.641\n",
      "iteration 61600, loss -17.176\n",
      "iteration 61700, loss -14.587\n",
      "iteration 61800, loss -12.388\n",
      "iteration 61900, loss -18.892\n",
      "iteration 62000, loss -12.999\n",
      "iteration 62100, loss -18.739\n",
      "iteration 62200, loss -14.474\n",
      "iteration 62300, loss -14.712\n",
      "iteration 62400, loss -16.406\n",
      "iteration 62500, loss -18.971\n",
      "iteration 62600, loss -12.364\n",
      "iteration 62700, loss -14.351\n",
      "iteration 62800, loss -19.701\n",
      "iteration 62900, loss -9.899\n",
      "iteration 63000, loss -17.045\n",
      "iteration 63100, loss -18.496\n",
      "iteration 63200, loss -19.059\n",
      "iteration 63300, loss -6.991\n",
      "iteration 63400, loss -12.759\n",
      "iteration 63500, loss -7.644\n",
      "iteration 63600, loss -18.867\n",
      "iteration 63700, loss -9.658\n",
      "iteration 63800, loss -7.674\n",
      "iteration 63900, loss -16.285\n",
      "iteration 64000, loss -9.212\n",
      "iteration 64100, loss -11.961\n",
      "iteration 64200, loss -12.532\n",
      "iteration 64300, loss -16.672\n",
      "iteration 64400, loss -11.783\n",
      "iteration 64500, loss -9.838\n",
      "iteration 64600, loss -19.426\n",
      "iteration 64700, loss -7.400\n",
      "iteration 64800, loss -14.647\n",
      "iteration 64900, loss -19.691\n",
      "iteration 65000, loss -17.990\n",
      "iteration 65100, loss -17.984\n",
      "iteration 65200, loss -18.786\n",
      "iteration 65300, loss -8.261\n",
      "iteration 65400, loss -9.265\n",
      "iteration 65500, loss -15.905\n",
      "iteration 65600, loss -21.355\n",
      "iteration 65700, loss -7.496\n",
      "iteration 65800, loss -13.334\n",
      "iteration 65900, loss -8.462\n",
      "iteration 66000, loss -7.261\n",
      "iteration 66100, loss -18.659\n",
      "iteration 66200, loss -8.187\n",
      "iteration 66300, loss -18.350\n",
      "iteration 66400, loss -15.436\n",
      "iteration 66500, loss -16.862\n",
      "iteration 66600, loss -20.042\n",
      "iteration 66700, loss -13.154\n",
      "iteration 66800, loss -14.874\n",
      "iteration 66900, loss -20.219\n",
      "iteration 67000, loss -17.064\n",
      "iteration 67100, loss -17.170\n",
      "iteration 67200, loss -12.640\n",
      "iteration 67300, loss -14.968\n",
      "iteration 67400, loss -10.436\n",
      "iteration 67500, loss -18.038\n",
      "iteration 67600, loss -15.574\n",
      "iteration 67700, loss -22.538\n",
      "iteration 67800, loss -13.633\n",
      "iteration 67900, loss -14.078\n",
      "iteration 68000, loss -15.576\n",
      "iteration 68100, loss -17.759\n",
      "iteration 68200, loss -17.298\n",
      "iteration 68300, loss -11.976\n",
      "iteration 68400, loss -14.379\n",
      "iteration 68500, loss -17.902\n",
      "iteration 68600, loss -13.806\n",
      "iteration 68700, loss -14.883\n",
      "iteration 68800, loss -9.264\n",
      "iteration 68900, loss -21.971\n",
      "iteration 69000, loss -24.293\n",
      "iteration 69100, loss -20.462\n",
      "iteration 69200, loss -11.347\n",
      "iteration 69300, loss -19.475\n",
      "iteration 69400, loss -10.962\n",
      "iteration 69500, loss -18.891\n",
      "iteration 69600, loss -13.486\n",
      "iteration 69700, loss -15.761\n",
      "iteration 69800, loss -9.090\n",
      "iteration 69900, loss -21.357\n",
      "iteration 70000, loss -24.792\n",
      "iteration 70100, loss -14.045\n",
      "iteration 70200, loss -12.392\n",
      "iteration 70300, loss -9.184\n",
      "iteration 70400, loss -19.468\n",
      "iteration 70500, loss -8.158\n",
      "iteration 70600, loss -6.725\n",
      "iteration 70700, loss -17.385\n",
      "iteration 70800, loss -11.438\n",
      "iteration 70900, loss -17.720\n",
      "iteration 71000, loss -18.014\n",
      "iteration 71100, loss -10.255\n",
      "iteration 71200, loss -21.322\n",
      "iteration 71300, loss -21.918\n",
      "iteration 71400, loss -10.245\n",
      "iteration 71500, loss -17.325\n",
      "iteration 71600, loss -17.539\n",
      "iteration 71700, loss -5.820\n",
      "iteration 71800, loss -22.240\n",
      "iteration 71900, loss -18.489\n",
      "iteration 72000, loss -17.822\n",
      "iteration 72100, loss -16.999\n",
      "iteration 72200, loss -22.797\n",
      "iteration 72300, loss -21.135\n",
      "iteration 72400, loss -16.318\n",
      "iteration 72500, loss -5.258\n",
      "iteration 72600, loss -18.814\n",
      "iteration 72700, loss -20.020\n",
      "iteration 72800, loss -19.007\n",
      "iteration 72900, loss -10.812\n",
      "iteration 73000, loss -13.901\n",
      "iteration 73100, loss -12.856\n",
      "iteration 73200, loss -12.863\n",
      "iteration 73300, loss -18.925\n",
      "iteration 73400, loss -18.928\n",
      "iteration 73500, loss -19.088\n",
      "iteration 73600, loss -13.186\n",
      "iteration 73700, loss -8.528\n",
      "iteration 73800, loss -7.603\n",
      "iteration 73900, loss -15.609\n",
      "iteration 74000, loss -18.214\n",
      "iteration 74100, loss -16.586\n",
      "iteration 74200, loss -16.056\n",
      "iteration 74300, loss -10.810\n",
      "iteration 74400, loss -10.180\n",
      "iteration 74500, loss -18.580\n",
      "iteration 74600, loss -20.102\n",
      "iteration 74700, loss -13.778\n",
      "iteration 74800, loss -20.455\n",
      "iteration 74900, loss -14.941\n",
      "iteration 75000, loss -18.127\n",
      "iteration 75100, loss -7.884\n",
      "iteration 75200, loss -8.975\n",
      "iteration 75300, loss -22.862\n",
      "iteration 75400, loss -18.669\n",
      "iteration 75500, loss -13.321\n",
      "iteration 75600, loss -22.656\n",
      "iteration 75700, loss -12.431\n",
      "iteration 75800, loss -13.597\n",
      "iteration 75900, loss -11.899\n",
      "iteration 76000, loss -19.966\n",
      "iteration 76100, loss -7.612\n",
      "iteration 76200, loss -15.114\n",
      "iteration 76300, loss -22.133\n",
      "iteration 76400, loss -19.209\n",
      "iteration 76500, loss -22.182\n",
      "iteration 76600, loss -17.128\n",
      "iteration 76700, loss -19.975\n",
      "iteration 76800, loss -19.177\n",
      "iteration 76900, loss -7.900\n",
      "iteration 77000, loss -15.359\n",
      "iteration 77100, loss -12.095\n",
      "iteration 77200, loss -8.827\n",
      "iteration 77300, loss -22.351\n",
      "iteration 77400, loss -10.693\n",
      "iteration 77500, loss -16.036\n",
      "iteration 77600, loss -21.774\n",
      "iteration 77700, loss -15.931\n",
      "iteration 77800, loss -11.795\n",
      "iteration 77900, loss -9.804\n",
      "iteration 78000, loss -16.338\n",
      "iteration 78100, loss -14.233\n",
      "iteration 78200, loss -21.849\n",
      "iteration 78300, loss -7.667\n",
      "iteration 78400, loss -13.986\n",
      "iteration 78500, loss -17.244\n",
      "iteration 78600, loss -11.274\n",
      "iteration 78700, loss -23.043\n",
      "iteration 78800, loss -20.463\n",
      "iteration 78900, loss -21.264\n",
      "iteration 79000, loss -15.268\n",
      "iteration 79100, loss -12.278\n",
      "iteration 79200, loss -12.996\n",
      "iteration 79300, loss -19.699\n",
      "iteration 79400, loss -16.905\n",
      "iteration 79500, loss -11.789\n",
      "iteration 79600, loss -18.720\n",
      "iteration 79700, loss -9.517\n",
      "iteration 79800, loss -11.021\n",
      "iteration 79900, loss -20.826\n",
      "iteration 80000, loss -14.582\n",
      "iteration 80100, loss -21.132\n",
      "iteration 80200, loss -17.660\n",
      "iteration 80300, loss -20.587\n",
      "iteration 80400, loss -17.540\n",
      "iteration 80500, loss -17.184\n",
      "iteration 80600, loss -21.827\n",
      "iteration 80700, loss -13.586\n",
      "iteration 80800, loss -9.657\n",
      "iteration 80900, loss -21.473\n",
      "iteration 81000, loss -24.005\n",
      "iteration 81100, loss -19.862\n",
      "iteration 81200, loss -17.580\n",
      "iteration 81300, loss -11.210\n",
      "iteration 81400, loss -16.206\n",
      "iteration 81500, loss -20.600\n",
      "iteration 81600, loss -7.562\n",
      "iteration 81700, loss -10.329\n",
      "iteration 81800, loss -16.816\n",
      "iteration 81900, loss -11.969\n",
      "iteration 82000, loss -14.729\n",
      "iteration 82100, loss -19.233\n",
      "iteration 82200, loss -21.686\n",
      "iteration 82300, loss -8.670\n",
      "iteration 82400, loss -22.946\n",
      "iteration 82500, loss -25.427\n",
      "iteration 82600, loss -15.904\n",
      "iteration 82700, loss -19.580\n",
      "iteration 82800, loss -22.053\n",
      "iteration 82900, loss -15.423\n",
      "iteration 83000, loss -8.658\n",
      "iteration 83100, loss -19.920\n",
      "iteration 83200, loss -19.506\n",
      "iteration 83300, loss -20.101\n",
      "iteration 83400, loss -6.729\n",
      "iteration 83500, loss -16.418\n",
      "iteration 83600, loss -21.428\n",
      "iteration 83700, loss -17.921\n",
      "iteration 83800, loss -18.863\n",
      "iteration 83900, loss -14.469\n",
      "iteration 84000, loss -16.364\n",
      "iteration 84100, loss -22.510\n",
      "iteration 84200, loss -11.028\n",
      "iteration 84300, loss -12.757\n",
      "iteration 84400, loss -7.164\n",
      "iteration 84500, loss -16.126\n",
      "iteration 84600, loss -15.326\n",
      "iteration 84700, loss -22.168\n",
      "iteration 84800, loss -8.565\n",
      "iteration 84900, loss -12.087\n",
      "iteration 85000, loss -10.257\n",
      "iteration 85100, loss -9.324\n",
      "iteration 85200, loss -19.239\n",
      "iteration 85300, loss -17.423\n",
      "iteration 85400, loss -18.927\n",
      "iteration 85500, loss -14.574\n",
      "iteration 85600, loss -24.175\n",
      "iteration 85700, loss -17.832\n",
      "iteration 85800, loss -17.475\n",
      "iteration 85900, loss -19.452\n",
      "iteration 86000, loss -13.332\n",
      "iteration 86100, loss -14.599\n",
      "iteration 86200, loss -10.382\n",
      "iteration 86300, loss -21.024\n",
      "iteration 86400, loss -16.344\n",
      "iteration 86500, loss -26.523\n",
      "iteration 86600, loss -22.467\n",
      "iteration 86700, loss -23.518\n",
      "iteration 86800, loss -11.380\n",
      "iteration 86900, loss -14.620\n",
      "iteration 87000, loss -19.758\n",
      "iteration 87100, loss -18.578\n",
      "iteration 87200, loss -19.477\n",
      "iteration 87300, loss -8.240\n",
      "iteration 87400, loss -22.956\n",
      "iteration 87500, loss -16.648\n",
      "iteration 87600, loss -14.741\n",
      "iteration 87700, loss -10.819\n",
      "iteration 87800, loss -9.490\n",
      "iteration 87900, loss -11.606\n",
      "iteration 88000, loss -21.338\n",
      "iteration 88100, loss -18.266\n",
      "iteration 88200, loss -16.489\n",
      "iteration 88300, loss -15.502\n",
      "iteration 88400, loss -16.539\n",
      "iteration 88500, loss -21.871\n",
      "iteration 88600, loss -15.267\n",
      "iteration 88700, loss -7.739\n",
      "iteration 88800, loss -13.171\n",
      "iteration 88900, loss -12.901\n",
      "iteration 89000, loss -18.326\n",
      "iteration 89100, loss -22.150\n",
      "iteration 89200, loss -19.670\n",
      "iteration 89300, loss -18.756\n",
      "iteration 89400, loss -19.829\n",
      "iteration 89500, loss -12.261\n",
      "iteration 89600, loss -11.107\n",
      "iteration 89700, loss -14.743\n",
      "iteration 89800, loss -20.576\n",
      "iteration 89900, loss -13.629\n",
      "iteration 90000, loss -9.170\n",
      "iteration 90100, loss -16.817\n",
      "iteration 90200, loss -17.428\n",
      "iteration 90300, loss -14.059\n",
      "iteration 90400, loss -21.067\n",
      "iteration 90500, loss -17.116\n",
      "iteration 90600, loss -12.726\n",
      "iteration 90700, loss -19.023\n",
      "iteration 90800, loss -21.675\n",
      "iteration 90900, loss -21.583\n",
      "iteration 91000, loss -26.753\n",
      "iteration 91100, loss -21.345\n",
      "iteration 91200, loss -18.768\n",
      "iteration 91300, loss -15.042\n",
      "iteration 91400, loss -18.049\n",
      "iteration 91500, loss -19.015\n",
      "iteration 91600, loss -19.062\n",
      "iteration 91700, loss -13.173\n",
      "iteration 91800, loss -19.773\n",
      "iteration 91900, loss -11.345\n",
      "iteration 92000, loss -6.959\n",
      "iteration 92100, loss -11.086\n",
      "iteration 92200, loss -22.920\n",
      "iteration 92300, loss -21.820\n",
      "iteration 92400, loss -10.575\n",
      "iteration 92500, loss -13.546\n",
      "iteration 92600, loss -15.853\n",
      "iteration 92700, loss -20.804\n",
      "iteration 92800, loss -15.038\n",
      "iteration 92900, loss -20.828\n",
      "iteration 93000, loss -22.786\n",
      "iteration 93100, loss -18.885\n",
      "iteration 93200, loss -21.240\n",
      "iteration 93300, loss -9.235\n",
      "iteration 93400, loss -14.086\n",
      "iteration 93500, loss -23.261\n",
      "iteration 93600, loss -9.081\n",
      "iteration 93700, loss -18.861\n"
     ]
    }
   ],
   "source": [
    "from utils import inpaint\n",
    "for i in range(50):\n",
    "    np_trainer.train(data_loader, epochs=1)\n",
    "    inpainting = inpaint(neuralprocess, img.unsqueeze(0), context_mask, device, RGB=False)\n",
    "    Image.fromarray(inpainting.squeeze().numpy()).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAA+klEQVR4nO2OMY5FUBiF70zGAkhIFBK1Rm4hkUjEHtiCim1oLMAOrMIWbqNAiCgQIQqF5kq4pniZKR7JTOapJr7uPzn/lwPAzc2lvB0jy7Js2+66DmMchmHf92VZviqtqkoUxe9znuckSU6f27b1fR8h9JR/HKu2bcuynKapJEkQQsMwVFVtmkYQhEdhXddxHHmeBwDUdX2Unix9gqZpCCFCSFGUR4IxLooiyzKGYVzXDYLgR8mvME1z27Y4jhmGucbIcdwwDPu+m6Z5Wnj/g9RxHJZlp2nK8/y1eV9omrYsCyFE1/VrjAAAz/MIIVEUURR1mfTm5uZf8AmQqGRIzwp8IAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28 at 0x7F64E24CD908>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpainting = inpaint(neuralprocess, img.unsqueeze(0), context_mask, device, RGB=False)\n",
    "Image.fromarray(inpainting.squeeze().numpy()).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(neuralprocess.state_dict(), '.\\Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
