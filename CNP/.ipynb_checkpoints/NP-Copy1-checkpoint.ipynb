{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.distributions.kl import kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim=2\n",
    "y_dim=1\n",
    "\n",
    "xspace = torch.linspace(-2, 2, 100).unsqueeze(0).transpose(0, 1)\n",
    "yspace = torch.linspace(-2, 2, 100).unsqueeze(0).transpose(0, 1)\n",
    "xx = torch.cat([xspace for i in range(yspace.size()[0])], dim=1)\n",
    "xx1 = xx.transpose(0, 1)\n",
    "linspace = torch.cat([xx1.unsqueeze(2), xx.unsqueeze(2)], dim=2).reshape(100**2,2)\n",
    "\n",
    "result = linspace[:,0].unsqueeze(0).transpose(0, 1)\n",
    "data = []\n",
    "for i in range(2000):\n",
    "    data.append([linspace, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, output_sizes):\n",
    "        super(encoder, self).__init__()\n",
    "        self.output_sizes = output_sizes\n",
    "        \n",
    "        self.fc1 = nn.Linear(x_dim + y_dim, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3_sigma = nn.Linear(10, self.output_sizes)\n",
    "        self.fc3_mu = nn.Linear(10, self.output_sizes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        #x and y are 1xn dimensional torch tensors\n",
    "        \n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "        l1 = self.relu(self.fc1(xy))\n",
    "        l2 = self.relu(self.fc2(l1))\n",
    "        logvar = self.fc3_sigma(l2)\n",
    "        mu = self.fc3_mu(l2)\n",
    "        sigma = torch.exp(logvar.mul(1/2))\n",
    "        \n",
    "        out_mu = torch.mean(mu, dim=0)\n",
    "        out_sigma = torch.mean(sigma, dim=0)\n",
    "        return out_mu.reshape(self.output_sizes, 1), out_sigma.reshape(self.output_sizes, 1), torch.distributions.Normal(out_mu, out_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, encoded_size):\n",
    "        super(decoder, self).__init__()\n",
    "        self.encoded_size = encoded_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.encoded_size + x_dim, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc21 = nn.Linear(10, 10)\n",
    "        self.fc3_mu = nn.Linear(10, y_dim)\n",
    "        self.fc3_sigma = nn.Linear(10, y_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, z, x):\n",
    "        #x and z are 1xn dimensional torch tensors\n",
    "        \n",
    "        zmulti = torch.cat([z for i in range(x.size()[0])], dim=0)\n",
    "    \n",
    "        xz = torch.cat([x, zmulti], dim=1)\n",
    "        \n",
    "        l1 = self.relu(self.fc1(xz))\n",
    "        l2 = self.relu(self.fc2(l1))\n",
    "        l21 = self.relu(self.fc21(l2))\n",
    "        out_mu = self.fc3_mu(l21)\n",
    "        out_logvar = self.fc3_sigma(l21)\n",
    "        out_sigma = torch.exp(out_logvar.mul(1/2))\n",
    "        \n",
    "        dist = torch.distributions.Normal(out_mu, out_sigma)\n",
    "        \n",
    "        return out_mu, out_sigma, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNP(nn.Module):\n",
    "    def __init__(self, encoded_size):\n",
    "        super(CNP, self).__init__()\n",
    "        self.encoded_size = encoded_size\n",
    "        self._encoder = encoder(encoded_size)\n",
    "        self._decoder = decoder(encoded_size)\n",
    "        \n",
    "    def forward(self, context_x, context_y, target_x, target_y=None):\n",
    "        en_mu, en_sigma, en_dist = self._encoder(context_x, context_y)\n",
    "        representation = en_dist.rsample().unsqueeze(0)\n",
    "        mu, sigma, dist = self._decoder(representation, target_x)\n",
    "        \n",
    "        if target_y is not None:\n",
    "            log_p = dist.log_prob(target_y.transpose(0, 1)).sum()\n",
    "            MSE = (mu - target_y.transpose(0,1)).pow(2).sum()\n",
    "        else:\n",
    "            log_p = None\n",
    "            MSE = None\n",
    "        return mu, sigma, log_p, en_dist, MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnp = CNP(3)\n",
    "optimizer = torch.optim.Adam(cnp.parameters(), lr=1e-3)\n",
    "num_test_maximum = 1\n",
    "plot_frequency = 1\n",
    "\n",
    "def lossf(log_p, z_prior, z_posterior, MSE):\n",
    "    KL =  kl_divergence(z_prior, z_posterior).prod()\n",
    "    return - log_p + KL\n",
    "\n",
    "\n",
    "def train(data, cnp, epochs, test_data=None):\n",
    "    cnp.train()\n",
    "    for epoch in range(epochs):\n",
    "        iterations = 0\n",
    "        total_loss = 0\n",
    "        for function in data:\n",
    "            optimizer.zero_grad()\n",
    "            num_points = function[0].size()[0]\n",
    "            perm = torch.randperm(num_points)\n",
    "            #num_context = np.random.randint(num_points - num_test_maximum, num_points)\n",
    "            num_context = num_test_maximum\n",
    "            context_x = function[0][perm][0:num_context]\n",
    "            context_y = function[1][perm][0:num_context]\n",
    "            test_x = function[0][perm][num_context:num_points]\n",
    "            test_y = function[1][perm][num_context:num_points]\n",
    "            \n",
    "            mu, sigma, log_p, en_dist, MSE = cnp(context_x, context_y, test_x, test_y)\n",
    "            loss = lossf(log_p, torch.distributions.Normal(torch.zeros(cnp.encoded_size), 1), en_dist, MSE)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss\n",
    "            iterations += 1\n",
    "            \n",
    "            if iterations % 200 == 0:\n",
    "                print(iterations)\n",
    "        \n",
    "        if epoch % plot_frequency == 0:\n",
    "            print(\"EPOCH: {}, LOSS {}\".format(epoch, total_loss))\n",
    "        '''\n",
    "            test_x = test_data[0].unsqueeze(0)\n",
    "            test_y = test_data[1].unsqueeze(0)\n",
    "            \n",
    "            linspace = torch.linspace(-2, 2, 100).unsqueeze(0)\n",
    "            print(linspace)\n",
    "            mu, sigma, _, _ = cnp(test_x, test_y, linspace)\n",
    "            lin = (linspace.numpy()[0])\n",
    "            low = np.array((mu-sigma).detach().numpy().T[0])\n",
    "            high = np.array((mu+sigma).detach().numpy().T[0])\n",
    "            plt.plot(linspace.numpy()[0], mu.detach().numpy())\n",
    "            plt.fill_between(lin, low, high, facecolor='#65c9f7', interpolate=True)\n",
    "            plt.scatter(test_x.numpy(), test_y.numpy(), c='black')\n",
    "            plt.show()\n",
    "        '''\n",
    "            \n",
    "def plot_priors(np, number):\n",
    "    norm = torch.distributions.Normal(torch.zeros(np.encoded_size), 1)\n",
    "    for i in range(number):\n",
    "        z = norm.rsample().unsqueeze(0).transpose(0, 1)\n",
    "        lin = torch.linspace(-2, 2, 100).unsqueeze(0)\n",
    "        ys, _, _ = np._decoder(z, lin)\n",
    "        yplot = ys.transpose(0,1).squeeze(0).detach().numpy()\n",
    "        xplot = lin.squeeze(0).numpy()\n",
    "        plt.plot(xplot, yplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-91f9b6e447d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-faeebd2055aa>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data, cnp, epochs, test_data)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMSE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoded_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(data, cnp, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xspace = torch.linspace(-2, 2, 100).unsqueeze(0).transpose(0, 1)\n",
    "yspace = torch.linspace(-2, 2, 100).unsqueeze(0).transpose(0, 1)\n",
    "xx = torch.cat([xspace for i in range(yspace.size()[0])], dim=1)\n",
    "xx1 = xx.transpose(0, 1)\n",
    "linspace = torch.cat([xx1.unsqueeze(2), xx.unsqueeze(2)], dim=2).reshape(100**2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = function[0].size()[0]\n",
    "perm = torch.randperm(num_points)\n",
    "num_context = np.random.randint(num_points - num_test_maximum, num_points)\n",
    "context_x = function[0][perm][0:num_context]\n",
    "context_y = function[1][perm][0:num_context]\n",
    "test_x = function[0][perm][num_context:num_points]\n",
    "test_y = function[1][perm][num_context:num_points]\n",
    "\n",
    "\n",
    "mu, sigma, log_p, en_dist, mse = cnp(context_x, context_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0021]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5859,  1.6768]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5859]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
