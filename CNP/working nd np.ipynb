{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.distributions.kl import kl_divergence\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(point, alpha):\n",
    "    return point.sum(dim=1).mul(alpha)\n",
    "\n",
    "num_datapoints = 2000\n",
    "alpha_range = [-2, 2]\n",
    "x_dimension = 2\n",
    "y_dimension = 1\n",
    "\n",
    "a = torch.linspace(-2, 2, 100)\n",
    "b = torch.linspace(-2, 2, 100)\n",
    "x_t = a.repeat(100).unsqueeze(0).transpose(0,1)\n",
    "y_t = b.repeat(100,1).t().contiguous().view(-1).unsqueeze(0).transpose(0,1)\n",
    "\n",
    "mesh = torch.cat([x_t, y_t], dim=1)\n",
    "datapoints = []\n",
    "for _ in range(num_datapoints):\n",
    "    num = np.random.uniform(alpha_range[0], alpha_range[1])\n",
    "    value = f(mesh, num).unsqueeze(0).transpose(0,1)\n",
    "    datapoints.append([mesh, value])\n",
    "\n",
    "points = np.random.choice(100**2, 50)\n",
    "testpoints = [mesh[points], f(mesh[points], 1).unsqueeze(0).transpose(0,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, output_sizes):\n",
    "        super(encoder, self).__init__()\n",
    "        self.output_sizes = output_sizes\n",
    "        \n",
    "        self.fc1 = nn.Linear(x_dimension+y_dimension, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, self.output_sizes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        #x and y are 1xn dimensional torch tensors\n",
    "        \n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "        l1 = self.relu(self.fc1(xy))\n",
    "        l2 = self.relu(self.fc2(l1))\n",
    "        R = self.fc3(l2)\n",
    "    \n",
    "        r_agg = torch.mean(R, dim=0).reshape(1, self.output_sizes)\n",
    "        return r_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class r_to_z(nn.Module):\n",
    "    def __init__(self, encoded_size):\n",
    "        super(r_to_z, self).__init__()\n",
    "        self.encoded_size = encoded_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.encoded_size, 10)\n",
    "        self.fc2_mu = nn.Linear(10, self.encoded_size)\n",
    "        self.fc2_logvar = nn.Linear(10, encoded_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, r):\n",
    "        mu = self.fc2_mu(self.relu(self.fc1(r))).reshape(3)\n",
    "        logvar = self.fc2_logvar(self.relu(self.fc1(r)))\n",
    "        std = torch.exp(logvar.mul(1/2)).reshape(3)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        \n",
    "        mu_out = mu.reshape(self.encoded_size, 1)\n",
    "        sigma_out = std.reshape(self.encoded_size, 1)\n",
    "        return mu_out, sigma_out, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, encoded_size):\n",
    "        super(decoder, self).__init__()\n",
    "        self.encoded_size = encoded_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.encoded_size + x_dimension, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc21 = nn.Linear(10, 10)\n",
    "        self.fc3_mu = nn.Linear(10, y_dimension)\n",
    "        self.fc3_sigma = nn.Linear(10, y_dimension)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, z, x):\n",
    "        #x and z are 1xn dimensional torch tensors\n",
    "        \n",
    "        zmulti = torch.cat([z for i in range(x.size()[0])], dim=1).transpose(0,1)\n",
    "        xz = torch.cat([x, zmulti], dim=1)\n",
    "        \n",
    "        l1 = self.relu(self.fc1(xz))\n",
    "        l2 = self.relu(self.fc2(l1))\n",
    "        l21 = self.relu(self.fc21(l2))\n",
    "        out_mu = self.fc3_mu(l21)\n",
    "        out_logvar = self.fc3_sigma(l21)\n",
    "        out_sigma = torch.exp(out_logvar.mul(1/2))\n",
    "        \n",
    "        dist = torch.distributions.Normal(out_mu, out_sigma)\n",
    "        \n",
    "        return out_mu, out_sigma, dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNP(nn.Module):\n",
    "    def __init__(self, encoded_size):\n",
    "        super(CNP, self).__init__()\n",
    "        self.encoded_size = encoded_size\n",
    "        self._encoder = encoder(encoded_size)\n",
    "        self._rz = r_to_z(encoded_size)\n",
    "        self._decoder = decoder(encoded_size)\n",
    "        \n",
    "    def forward(self, context_x, context_y, target_x, target_y=None):\n",
    "        en_mu, en_sigma, en_dist = self._rz(self._encoder(context_x, context_y))\n",
    "        if target_y is not None:\n",
    "            t_en_mu, t_en_sigma, t_en_dist = self._rz(self._encoder(target_x, target_y))\n",
    "            representation = t_en_dist.rsample().unsqueeze(0).transpose(0,1)\n",
    "        else:\n",
    "            representation = en_dist.rsample().unsqueeze(0).transpose(0, 1)\n",
    "            t_en_dist = None\n",
    "\n",
    "        mu, sigma, dist = self._decoder(representation, target_x)\n",
    "        \n",
    "        if target_y is not None:\n",
    "            log_p = dist.log_prob(target_y)\n",
    "            log_p = log_p.sum()\n",
    "            \n",
    "            MSE = (mu - target_y.transpose(0,1)).sum()\n",
    "        else:\n",
    "            log_p = None\n",
    "            MSE = None\n",
    "        return mu, sigma, log_p, en_dist, t_en_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnp = CNP(3)\n",
    "optimizer = torch.optim.Adam(cnp.parameters(), lr=3e-3)\n",
    "num_test_maximum = 0.8*100**2\n",
    "plot_frequency = 1\n",
    "alpha = torch.tensor(1)\n",
    "\n",
    "def compute_kernel(x, y):\n",
    "    return (-(x - y).pow(2)).exp()\n",
    "\n",
    "def MMD(z_prior, z_posterior, num_samples):\n",
    "    target_samples_1 = z_prior.sample((num_samples,))\n",
    "    context_samples_1 = z_posterior.sample((num_samples,))\n",
    "    target_samples_2 = z_prior.sample((num_samples,))\n",
    "    context_samples_2 = z_posterior.sample((num_samples,))\n",
    "    \n",
    "    t_kernel = compute_kernel(target_samples_1, target_samples_2)\n",
    "    c_kernel = compute_kernel(context_samples_1, context_samples_2)\n",
    "    ct_kernel = compute_kernel(target_samples_1, context_samples_1) + compute_kernel(target_samples_2, context_samples_2)\n",
    "    return (t_kernel + c_kernel - ct_kernel).mean()\n",
    "    \n",
    "\n",
    "def lossf(log_p, z_prior, context_z_posterior, test_z_posterior, alpha):\n",
    "    KL =  MMD(test_z_posterior, context_z_posterior, 10).prod()\n",
    "    return -log_p + alpha*KL\n",
    "\n",
    "\n",
    "def train(data, cnp, epochs, test_data):\n",
    "    cnp.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        iteration = 0\n",
    "        for function in data:\n",
    "            optimizer.zero_grad()\n",
    "            num_points = function[0].size()[0]\n",
    "            perm = torch.randperm(num_points)\n",
    "            num_context = np.random.randint(num_points - num_test_maximum, num_points)\n",
    "            context_x = function[0][perm][0:num_context]\n",
    "            context_y = function[1][perm][0:num_context]\n",
    "            test_x = function[0][perm][num_context:num_points]\n",
    "            test_y = function[1][perm][num_context:num_points]\n",
    "            mu, sigma, log_p, en_dist, t_en_dist = cnp(context_x, context_y, test_x, test_y)\n",
    "            loss = lossf(log_p, torch.distributions.Normal(torch.zeros(cnp.encoded_size), 1), en_dist, t_en_dist, alpha)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss\n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % 200 == 0:\n",
    "                print(iteration)\n",
    "        \n",
    "        if epoch % plot_frequency == 0:\n",
    "            print(\"EPOCH: {}, LOSS {}\".format(epoch, total_loss))\n",
    "            \n",
    "            fig = pyplot.figure()\n",
    "            ax = Axes3D(fig)\n",
    "            \n",
    "            test_x = test_data[0]\n",
    "            test_y = test_data[1]\n",
    "            \n",
    "            linspace = mesh\n",
    "            \n",
    "            mu, sigma, _, _, _ = cnp(test_x, test_y, linspace)\n",
    "            lin = (linspace.numpy()[0])\n",
    "            low = np.array((mu-sigma).detach().numpy().T[0])\n",
    "            high = np.array((mu+sigma).detach().numpy().T[0])\n",
    "            \n",
    "            ax.scatter(linspace.numpy()[:,0], linspace.numpy()[:,1], mu.detach().numpy(), c='blue', alpha=0.05)\n",
    "            ax.scatter(test_x.numpy()[:,0], test_x.numpy()[:,1], test_y.numpy(), c='black', alpha=0.5)\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "def plot_priors(np, number):\n",
    "    norm = torch.distributions.Normal(torch.zeros(np.encoded_size), 1)\n",
    "    for i in range(number):\n",
    "        z = norm.rsample().unsqueeze(0).transpose(0, 1)\n",
    "        lin = torch.linspace(-2, 2, 100).unsqueeze(0)\n",
    "        ys, _, _ = np._decoder(z, lin)\n",
    "        yplot = ys.transpose(0,1).squeeze(0).detach().numpy()\n",
    "        xplot = lin.squeeze(0).numpy()\n",
    "        plt.plot(xplot, yplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(datapoints, cnp, 100, testpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
